{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization in python scientific computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Linux system: it forks the sub processes, meaning each sub process will inheret the current state of parent program, they do have a 4GB argument restriction, but other than that, it is pretty flexible since no much pickling process got involved. The inherent parental variable is \"copy-on-modify\"\n",
    "\n",
    "- In other system, it spawns sub processes, so a new interpreter gets launched, quite a lot pickling process involved, so like file IO, generating figures can not be parallelized, better to have a clear input and output in the enqueue and dequeue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pool.map will take the iteratble, make them a list, divide list to chunk and finally picklize the chunk to each sub-process (worker).\n",
    "2. it runs out of order, but output will preserve the order, faster subprocess will have to wait slower one\n",
    "3. result is just a list\n",
    "4. result won't immediately return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "r = pool.map(func,querys)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pool.map_async will immediately return AsyncResult object and main program can proceed, but can not actually call AsyncResult.get() until all the subprocesses finish, so again, it preserve the order\n",
    "2. you can mannually call r.wait() or pool.close() + pool.join() to make sure main program won't go until all workers finish. r is the AsyncResult obejct, but pool is the Pool object, Pool.close() instruct Pool to not take any new jobs, Pool.join() to instruct Pool to wait until all subprocesses have finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "r = pool.map_async(func,querys)\n",
    "print(r.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pool.imap will not make them a list and divide to chunk, it will just take one from the queue, send to subprocess, and one after the another\n",
    "2. it will preserve the order as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "r = pool.imap(func,querys)\n",
    "for item in r:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pool.imap_unordered will not preserve the order, return the result immediately, and you can access it, main program will continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "r = pool.imap_unordered(func,querys)\n",
    "for item in r:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pool.apply just can take additional argument\n",
    "2. pool.apply_async, the same as pool.map_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_apply(x,y):\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "y = 5\n",
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "r = [pool.apply(func_apply,args=(x,y)) for x in querys]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "y = 5\n",
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "r = [pool.apply_async(func_apply,args=(x,y)) for x in querys]\n",
    "for item in r:\n",
    "    print(item.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
